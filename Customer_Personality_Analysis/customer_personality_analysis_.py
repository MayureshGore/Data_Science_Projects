# -*- coding: utf-8 -*-
"""Customer_Personality_Analysis_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZgjExnNptgPCQ7BOYJr5ZG1tloQ7arDX

# **Customer Personality Analysis**

# **Table of Contents**

* Problem Statement

* Project Objectives

* Importing Libraries and Read In Dataset

* Feature Engineering

* Handle Missing Values

* Visualizing the Data

* Conclusions on Objective 1

* Dimensionality Reduction

* Clustering Customers

* Profiling

* About the Clusters

* Conclusions on Objective 2

* Create Models

* Ensemble the Models

* Conclusions on Objective 3

* Deployment

# **Problem Statement**
Customer Personality Analysis is a detailed analysis of a company’s ideal customers. It helps a business to better understand its customers and makes it easier for them to modify products according to the specific needs, behaviors and concerns of different types of customers. Customer personality analysis helps a business to modify its product based on its target customers from different types of customer segments. For example, instead of spending money to market a new product to every customer in the company’s database, a company can analyze which customer segment is most likely to buy the product and then market the product only on that particular segment.

# **INTRODUCTION**
**This project aims to analyze customer behavior and preferences using a dataset from a marketing campaign. The dataset contains various customer attributes such as age, education, income, and spending habits on different products. By leveraging data science techniques, we intend to gain insights into customer segmentation, predict customer responses to marketing campaigns, and estimate customer lifetime value (CLV).**

# **Project Objectives**
**1.** **Determine customer traits and behaviors**

**2. Group similar customers based on traits and behaviors**

**3. Create predictive model to predict which customers will respond to marketting campaigns**

**Objective 1,**

**determine customer traits and behaviors.**

# **Importing Libraries and Read In Dataset**
"""

import numpy as np
import pandas as pd
import matplotlib
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, normalize
!pip install dataprep
from dataprep.eda import plot, plot_correlation, create_report, plot_missing
from sklearn.preprocessing import LabelEncoder
from sklearn.decomposition import PCA
from sklearn.cluster import AgglomerativeClustering
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb
from sklearn.metrics import accuracy_score, recall_score
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import cross_val_score, cross_val_predict, train_test_split
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, roc_curve, roc_auc_score

data=pd.read_csv('/content/marketing_campaign1.csv')
data.head()

"""# **About Each Attribute**

**People**

* ID: Customer's unique identifier

* Year_Birth: Customer's birth year

* Education: Customer's education level

* Marital_Status: Customer's marital status

* Income: Customer's yearly household income

* Kidhome: Number of children in customer's household

* Teenhome: Number of teenagers in customer's household

* Dt_Customer: Date of customer's enrollment with the company

* Recency: Number of days since customer's last purchase

* Complain: 1 if customer complained in the last 2 years, 0 otherwise

**Products**

* MntWines: Amount spent on wine in last 2 years

* MntFruits: Amount spent on fruits in last 2 years

* MntMeatProducts: Amount spent on meat in last 2 years

* MntFishProducts: Amount spent on fish in last 2 years

* MntSweetProducts: Amount spent on sweets in last 2 years

* MntGoldProds: Amount spent on gold in last 2 years

**Promotion**

* NumDealsPurchases: Number of purchases made with a discount

* AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise

* AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise

* AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise

* AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise

* AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise

* Response: 1 if customer accepted the offer in the last campaign, 0 otherwise


**Place**

* NumWebPurchases: Number of purchases made through the company’s web site

* NumCatalogPurchases: Number of purchases made using a catalogue

* NumStorePurchases: Number of purchases made directly in stores

* NumWebVisitsMonth: Number of visits to company’s web site in the last month

"""

data.shape

data.info()

#checking missing values
data.isnull().sum()

#To remove the NA values
data = data.dropna()
print("The total number of data-points after removing the rows with missing values are:", len(data))

data.isnull().sum()

"""#Dt_Customer" indicates the day particular customer registered with the firm. Check the newest and oldest recorded dates."""

data["Dt_Customer"] = pd.to_datetime(data["Dt_Customer"])
dates = []
for i in data["Dt_Customer"]:
    i = i.date()
    dates.append(i)
#Dates of the newest and oldest recorded customer
print("The newest customer's enrolment date in the records:",max(dates))
print("The oldest customer's enrolment date in the records:",min(dates))

"""#Exploring the unique values in the categorical features to get a clear idea of the data."""

print("Total categories in the feature Marital_Status:\n", data["Marital_Status"].value_counts(), "\n")
print("Total categories in the feature Education:\n", data["Education"].value_counts())

"""# **Feature Engineering**

#Calculating “Age” of a customer by the “Year_Birth”. We will calculate age till 2014. As we have data till 2014.
"""

data['Age'] = 2014 - data['Year_Birth']

#Total spendings on various items
data["Spent"] = data["MntWines"]+ data["MntFruits"]+ data["MntMeatProducts"]+ data["MntFishProducts"]+ data["MntSweetProducts"]+ data["MntGoldProds"]

#Deriving living situation by marital status"Alone"
data["Living_With"]=data["Marital_Status"].replace({"Married":"Partner", "Together":"Partner", "Absurd":"Alone", "Widow":"Alone", "YOLO":"Alone", "Divorced":"Alone", "Single":"Alone",})

#Feature indicating total children living in the household
data["Children"]=data["Kidhome"]+data["Teenhome"]

#Feature for total members in the householde
data["Family_Size"] = data["Living_With"].replace({"Alone": 1, "Partner":2})+ data["Children"]

#Feature pertaining parenthood
data["Is_Parent"] = np.where(data.Children> 0, 1, 0)

#Segmenting education levels in three groups
data["Education"]=data["Education"].replace({"Basic":"Undergraduate","2n Cycle":"Undergraduate", "Graduation":"Graduate", "Master":"Postgraduate", "PhD":"Postgraduate"})

#renaming the columns
data=data.rename(columns={"MntWines": "Wines","MntFruits":"Fruits","MntMeatProducts":"Meat","MntFishProducts":"Fish","MntSweetProducts":"Sweets","MntGoldProds":"Gold"})

#Dropping some of the redundant features
to_drop = ["Marital_Status", "Dt_Customer", "Z_CostContact", "Z_Revenue", "Year_Birth", "ID"]
data = data.drop(to_drop, axis=1)

data.head()

days = []
d1 = max(dates) #taking it to be the newest customer
for i in dates:
    delta = d1 - i
    days.append(delta)
data["Customer_For"] = days
data["Customer_For"] = pd.to_numeric(data["Customer_For"], errors="coerce")

plt.figure(figsize=(12,5))

sns.distplot(data["Customer_For"],color = 'turquoise')

#To plot some selected features
#Setting up colors prefrences
sns.set(rc={"axes.facecolor":"#FFF9ED","figure.facecolor":"#FFF9ED"})
pallet = ["#682F2F", "#9E726F", "#D6B2B1", "#B9C0C9", "#9F8A78", "#F3AB60"]
#Plotting following features
To_Plot = [ "Income", "Recency", "Customer_For", "Age", "Spent", "Is_Parent"]
print("Reletive Plot Of Some Selected Features: A Data Subset")
plt.figure()
sns.pairplot(data[To_Plot], hue= "Is_Parent",palette= (["#682F2F","#F3AB60"]))
#Taking hue
plt.show()

data = data[(data["Age"]<90)]
data = data[(data["Income"]<600000)]
print("The total number of data-points after removing the outliers are:", len(data))

data.columns

"""#Checking distribution for the data"""

plt.figure(figsize = (12, 12))
plt.subplots_adjust(hspace = 1.5, wspace=0.5)

plt.subplot(5, 3, 1)
sns.histplot(data, x = 'Income', kde = True, bins = 20)
plt.title("Distribution of Income")

plt.subplot(5, 3, 2)
sns.histplot(data, x = 'Recency', kde = True, bins = 20)
plt.title("Distribution of Recency")

plt.subplot(5, 3, 3)
sns.histplot(data, x = "Wines", kde = True, bins = 20)
plt.title("Distribution of Amount spent on Wines")

plt.subplot(5, 3, 4)
sns.histplot(data, x = 'Fruits', kde = True, bins = 20)
plt.title("Distribution of amount spent on Fruits")

plt.subplot(5, 3, 5)
sns.histplot(data, x = 'Meat', kde = True, bins = 20)
plt.title("Distribution of amount spent on Meat")

plt.subplot(5, 3, 6)
sns.histplot(data, x = 'Fish', kde = True, bins = 20)
plt.title("Distribution of amount spent on Fish")

plt.subplot(5, 3, 7)
sns.histplot(data, x = 'Sweets', kde = True, bins = 20)
plt.title("Distribution of amount spent on Sweets")

plt.subplot(5, 3, 8)
sns.histplot(data, x = 'Gold', kde = True, bins = 20)
plt.title("Distribution of amount spent on Gold")

plt.subplot(5, 3, 9)
sns.histplot(data, x = 'NumDealsPurchases', kde = True, bins = 20)
plt.title("Distribution of Deal Purchased")

plt.subplot(5, 3, 10)
sns.histplot(data, x = 'NumWebPurchases', kde = True, bins = 20)
plt.title("Distribution of from Web Purchase")

plt.subplot(5, 3, 11)
sns.histplot(data, x = 'NumCatalogPurchases', kde = True, bins = 20)
plt.title("Distribution of from Catalog Purchase")

plt.subplot(5, 3, 12)
sns.histplot(data, x = 'NumStorePurchases', kde = True, bins = 20)
plt.title("Distribution of from Store Purchase")

plt.subplot(5, 3, 13)
sns.histplot(data, x = 'NumWebVisitsMonth', kde = True, bins = 20)
plt.title("Distribution of Visit per Month")

plt.subplot(5, 3, 14)
sns.histplot(data, x = 'Age', kde = True, bins = 20)
plt.title("Distribution of Customer Age")

plt.subplot(5, 3, 15)
sns.histplot(data, x = 'Spent', kde = True, bins = 20)
plt.title("Distribution of amount spent")

plt.show()

"""#From the distribution plots we can see most of the distributions are Right skewed"""

sns.set_theme(style="white")
plt.figure(figsize=(8,8))
plt.title("how education impact on spent", fontsize=24)
ax = sns.barplot(x ='Education',y='Spent',data=data,palette ="rainbow")
#expenses of post graduates are more .

sns.set_theme(style="white")
plt.figure(figsize=(8, 8))
plt.title("How Kids Impact on Spending", fontsize=24)
ax = sns.barplot(x='Children', y='Spent', data=data, palette="rainbow")
plt.show()

# We can observe the parents with one kid is spending more than the parents with 2 or 3
plt.figure(figsize = (12, 6))
sns.scatterplot(data, x = 'Age', y = 'Spent')
plt.title("Spent vs Age")
plt.show()
print(f"\nCorrelation between Age and Spent: {data['Age'].corr(data['Spent'])}")

"""#From Spent vs Age plot, we can see that as Age increase, Spent also increases. Correlation score is 0.115."""

plt.figure(figsize = (12, 6))
sns.scatterplot(data, x = 'Income', y = 'Spent')
plt.title("Spent vs Income")
plt.grid(False)
plt.show()
print(f"\nCorrelation between Income and Spent: {data['Income'].corr(data['Spent'])}")

"""#From Spent va Income plot, we can see that as theIncome increases, Spent increases drastically. Correlation score is 0.79.

#Converting categorial variable into Numerical variable
"""

#checking the categorical variables in the data
a = (data.dtypes == 'object')
object_cols = list(a[a].index)
print("Categorical variables in the dataset:", object_cols)

LE = LabelEncoder()
for i in object_cols:
  data[i] = data[[i]].apply(LE.fit_transform)

data.head()

cmap = "coolwarm"
#correlation matrix
corrmat= data.corr()
plt.figure(figsize=(20,20))
sns.heatmap(corrmat,annot=True, cmap=cmap, center=0)

"""# **Conclusions On Objective 1**
**Total amount spent on products,** especially wines and meats, are very highly correlated with whether the customer responded to the marketing campaign. However, amount spent on gold, fish, sweets and fish were not as correlated. This could be due to the nature of the most recent marketing campaign - perhaps the store was trying to sell meat and wine in the most recent campaign.

**Catalog purchases correlate with response** to the current marketing campaign where as in store, online, and deal purchases have very little to no correlation. This may be due to the medium that the marketing campaign was using - maybe it was not displayed in store/online but was in all the catalogs. Another possibility is that those customers who perform catalog purchases are more influenced by the campaigns

**Customers with smaller family size responded better** to the marketing campaign. Maybe the customers without family had more money to spend on the products in the campaign or the products in the campaign were for signle customers (like alcohol and party supplies). Without further inforamation on the details on the campaign it is hard to say.


**Finally, of note is Age and Complainin**g had virtually 0 correlation with response. This shows that the campaign did a good job of catering to **all age groups and that customers who complained in the past continued bussiness at the store.**

**Objective 2, Group similar customers based on traits and behaviors.**
"""

#Creating a copy of data
ds = data.copy()
# creating a subset of dataframe by dropping the features on deals accepted and promotions
cols_del = ['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1','AcceptedCmp2', 'Complain', 'Response']
ds = ds.drop(cols_del, axis=1)
#Scaling
scaler = StandardScaler()
scaler.fit(ds)
scaled_ds = pd.DataFrame(scaler.transform(ds),columns= ds.columns )
print("All features are now scaled")

#Scaled data to be used for reducing the dimensionality
print("Dataframe to be used for further modelling:")
scaled_ds.head()

"""#K-Means"""

wcss=[]
for i in range (1,11):
 kmeans=KMeans(n_clusters=i,init='k-means++',random_state=42)
 kmeans.fit(scaled_ds)
 wcss.append(kmeans.inertia_)
plt.figure(figsize=(16,8))
plt.plot(range(1,11),wcss, 'bx-')
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

"""#Here we can observe from the plot cluster = 2
#we can also check with silhouette score
"""

silhouette_scores = []
for i in range(2,10):
    m1=KMeans(n_clusters=i, random_state=42)
    c = m1.fit_predict(scaled_ds)
    silhouette_scores.append(silhouette_score(scaled_ds, m1.fit_predict(scaled_ds)))
plt.bar(range(2,10), silhouette_scores)
plt.xlabel('Number of clusters', fontsize = 20)
plt.ylabel('S(i)', fontsize = 20)
plt.show()

silhouette_scores

sc=max(silhouette_scores)
number_of_clusters=silhouette_scores.index(sc)+2
print("Number of Cluster Required is : ", number_of_clusters)

# Training  using K-Means Algorithm.

kmeans=KMeans(n_clusters=number_of_clusters, random_state=42).fit(scaled_ds)
pred=kmeans.predict(scaled_ds)

# Appending those cluster value into main dataframe

data['cluster k-means'] = pred + 1

data.head()

#Plotting countplot of clusters
pal = ["#682F2F","#B9C0C9"]
pl = sns.countplot(x=data["cluster k-means"], palette= pal)
pl.set_title("Distribution Of The Clusters")
plt.show()

sns.scatterplot(x='Spent', y='Income', hue='cluster k-means', data=data)
plt.title('Scatter Plot of Spent vs. Income', fontsize=16)
plt.show()

"""#PCA WITH AGGLOMERATIVE CLUSTERING"""

#Initiating PCA to reduce dimentions aka features to 3
pca = PCA(n_components=3)
pca.fit(scaled_ds)
PCA_ds = pd.DataFrame(pca.transform(scaled_ds), columns=(["col1","col2", "col3"]))
PCA_ds.describe().T

PCA_ds.head()

#A 3D Projection Of Data In The Reduced Dimension
x =PCA_ds["col1"]
y =PCA_ds["col2"]
z =PCA_ds["col3"]
#To plot
fig = plt.figure(figsize=(10,8))
ax = fig.add_subplot(111, projection="3d")
ax.scatter(x,y,z, c="maroon", marker="o" )
ax.set_title("A 3D Projection Of Data In The Reduced Dimension")
plt.show()

wcss=[]
for i in range (1,11):
 kmeans=KMeans(n_clusters=i,init='k-means++',random_state=42)
 kmeans.fit(PCA_ds)
 wcss.append(kmeans.inertia_)
plt.figure(figsize=(16,8))
plt.plot(range(1,11),wcss, 'bx-')
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

"""#when we performed pca and supplied it to k means error has been reduced reduced"""

#Initiating agglomerative clustering model
AC = AgglomerativeClustering(n_clusters=2)
yhat_AC = AC.fit_predict(PCA_ds)
PCA_ds["Clusters"] = yhat_AC

#adding cluster attribute the original data frame
data["Cluster_Agglo"] = yhat_AC + 1

data.head()

sns.scatterplot(x='Spent', y='Income', hue='Cluster_Agglo', data=data)
plt.title('Scatter Plot of Spent vs. Income', fontsize=16)
plt.show()

sns.scatterplot(x='Children', y='Income', hue='Cluster_Agglo', data=data)
plt.title('Scatter Plot of Spent vs. Income', fontsize=16)
plt.show()

sns.scatterplot(x='Income', y='Age', hue='Cluster_Agglo', data=data)
plt.title('Scatter Plot of Spent vs. Income', fontsize=16)
plt.show()

sns.scatterplot(x='Living_With', y='Income', hue='Cluster_Agglo', data=data)
plt.title('Scatter Plot of Spent vs. Income', fontsize=16)
plt.show()

sns.scatterplot(x='Living_With', y='Spent', hue='Cluster_Agglo', data=data)
plt.title('Scatter Plot of Spent vs. Income', fontsize=16)
plt.show()

sns.scatterplot(x='Income', y='Age', hue='Cluster_Agglo', data=data)
plt.title('Scatter Plot of Spent vs. Income', fontsize=16)
plt.show()

fig = plt.figure(figsize=(16, 14))
ax = fig.add_subplot(111, projection='3d', label="bla")

ax.scatter(x, y, z, s=40, c=PCA_ds["Clusters"], marker='o')
ax.set_title("The plot of the customers")

plt.show()

#Plotting the number of deals purchased
plt.figure()
pl=sns.boxenplot(y=data["NumDealsPurchases"],x=data["cluster k-means"], palette= pal)
pl.set_title("Number of Deals Purchased")
plt.show()

plt.figure()
pl=sns.swarmplot(x=data["cluster k-means"], y=data["Spent"], color= "#CBEDDD", alpha=0.5 )
pl=sns.boxenplot(x=data["cluster k-means"], y=data["Spent"], palette=pal)
plt.show()

#Creating a feature to get a sum of accepted promotions
data["Total_Promos"] = data["AcceptedCmp1"]+ data["AcceptedCmp2"]+ data["AcceptedCmp3"]+ data["AcceptedCmp4"]+ data["AcceptedCmp5"]
#Plotting count of total campaign accepted.
plt.figure()
pl = sns.countplot(x=data["Total_Promos"],hue=data["cluster k-means"], palette= pal)
pl.set_title("Count Of Promotion Accepted")
pl.set_xlabel("Number Of Total Accepted Promotions")
plt.show()

"""## **PROFILING**"""

Personal = [ "Kidhome","Teenhome","Customer_For", "Age", "Children", "Family_Size", "Is_Parent", "Education","Living_With"]

for i in Personal:
    plt.figure()
    sns.jointplot(x=data[i], y=data["Spent"], hue =data["cluster k-means"], kind="kde", palette=pal)
    plt.show()

"""**About cluster 1:**

Low Spent
1. There are maximum 2 keeds at home.
2. There are also teenagers at home.
3. Most are olders and parents.
4. (0 to 3) childrens at home.
5. At the max have 5 members in the family and at least 1.
6. Parents are a subset of this group.
7. Highly qualified.

**About cluster 2:**

Heigh Spent
1. There are no keeds at home.
2. Most have teenager at home.
3. Most are youngesters.
4. (0 to 1) childrens at home.
5. At the max have 3 members in the family and at least 1.
6. Single Parents are a subset of this group.
7. Most of them are undergraduate and graduate.

# **Conclusions on Objective 2**
**The majority of customers are low spenders with big families** and not likely to respond to a campaign. This group of customers have lower incomes on average (compared to other customer groups) and probably do not have much money to spend on extra products. Marketing targeting this group will be difficult due to these reasons.

**Customers who deviate from the norm, are more likely to respond to marketing.**  this group make more and spend more money than the majority of customers. Marketing targeting these groups have a higher chance of success. This could be accomplished using the web or catalogs.

**Now, objective 3, create predictive model to predict which customers will respond to marketting campaigns.**

# **Create Models**
"""

import pandas as pd
# Define features (X) and target variable (Y)
X = data[['Education', 'Income', 'Kidhome', 'Teenhome', 'Spent', 'Wines',
          'Fruits', 'Meat', 'Fish', 'Sweets', 'Gold',
          'NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases',
          'NumWebVisitsMonth', 'Recency', 'Age','Complain',
          'Living_With', 'Children', 'Family_Size', 'Is_Parent', 'Customer_For']]
Y = data[['Response']]

# Preprocess the data
scaler = StandardScaler()
scaler.fit(ds)
scaled_ds = pd.DataFrame(scaler.transform(ds),columns= ds.columns )
X_scaled_ds = scaler.fit_transform(X)

# Split the dataset into training and testing sets (80% training, 20% testing)
x_train, x_test, y_train, y_test = train_test_split(X_scaled_ds, Y, test_size=0.2, random_state=42)

from imblearn.over_sampling import RandomOverSampler

# Instantiate RandomOverSampler
ros = RandomOverSampler(random_state=42)

# Fit and apply RandomOverSampler to balance the dataset
X_balanced, Y_balanced = ros.fit_resample(X, Y)

# Convert the balanced data back to a dataframe if needed
balanced_data = pd.concat([pd.DataFrame(X_balanced, columns=X.columns),
                           pd.DataFrame(Y_balanced, columns=['Response'])], axis=1)

# Fit logistic regression model
log_reg = LogisticRegression(random_state=42)
log_reg.fit(x_train, y_train)

log_reg_accuracy = accuracy_score(y_test, log_reg.predict(x_test))
print("Logistic Regression Accuracy:", log_reg_accuracy)

# Fit SVM model
svm_model = SVC(kernel='linear', random_state=42)
svm_model.fit(x_train, y_train)

svm_accuracy = accuracy_score(y_test, svm_model.predict(x_test))
print("SVM Accuracy:", svm_accuracy)

# Fit KNN model
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(x_train, y_train)

knn_accuracy = accuracy_score(y_test, knn_model.predict(x_test))
print("KNN Accuracy:", knn_accuracy)

# Instantiate and train a Random Forest classifier
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(x_train, y_train)

# Make predictions on the test set
y_pred = rf_model.predict(x_test)

# Calculate accuracy and recall scores
accuracy = accuracy_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy:", accuracy)
print("Recall:", recall)
print("Precision:", precision)
print("F1 Score:", f1)

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, cross_val_predict
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Generate synthetic data for example
X, Y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# Split data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Instantiate a Random Forest classifier
rf_model = RandomForestClassifier(random_state=42)

# Fit the model
rf_model.fit(X_train, Y_train)

# Get predicted probabilities using cross-validation
y_scores = cross_val_predict(rf_model, X, Y, cv=5, method='predict_proba')[:, 1]  # Probabilities for class 1

# Calculate false positive rate (FPR) and true positive rate (TPR)
fpr, tpr, thresholds = roc_curve(Y, y_scores)

# Calculate AUC score
auc_score = roc_auc_score(Y, y_scores)
print("AUC Score:", auc_score)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label='ROC Curve (AUC = {:.2f})'.format(auc_score))
plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

# Fit Naive Bayes model
nb_model = GaussianNB()
nb_model.fit(x_train, y_train)

nb_accuracy = accuracy_score(y_test, nb_model.predict(x_test))
print("Naive Bayes Accuracy:", nb_accuracy)

# Define and fit Gradient Boosting model
gbm_model = GradientBoostingClassifier(random_state=42)
gbm_model.fit(x_train, y_train)

# Evaluate GBM model
gbm_accuracy = accuracy_score(y_test, gbm_model.predict(x_test))
print("Gradient Boosting Accuracy:", gbm_accuracy)

# Convert data to DMatrix format for XGBoost
dtrain = xgb.DMatrix(x_train, label=y_train)
dtest = xgb.DMatrix(x_test, label=y_test)

# Define parameters for XGBoost
params = {
    'objective': 'binary:logistic',
    'eval_metric': 'error',
    'max_depth': 3,
    'learning_rate': 0.1,
    'n_estimators': 100,
    'seed': 42
}

# Train XGBoost model
xgb_model = xgb.train(params, dtrain)

# Make predictions using XGBoost model
xgb_preds = xgb_model.predict(dtest)
xgb_accuracy = accuracy_score(y_test, (xgb_preds > 0.5).astype(int))
print("XGBoost Accuracy:", xgb_accuracy)

"""# **Conclusions on Objective 3**
**Random_forest_model_Accuracy: 83.97%**

**Recall: 0.1688311688311688**

**Precision: 0.65**

**F1 Score: 0.26804123711340205**

**AUC_Score: 95%**

 This is a reasonable balance between the two metrics and will allow the store to identify and target the majority of customers who will respond to marketing while not having to spend an excess of resources targeting large amounts of customers who will not respond. If the store was willing to spend a bit more on marketing, the ensemble model could be modified to identify customers who will respond.

**This dataset may not be complex enough. Customers are complex**. There are a variety of reasons why a customer would respond to marketing and the dataset used here only includes a small fraction of all variables that need to be considered. That being said, some more feature that could be useful if provided would be: what items are being marketed for each of the campaigns, times of year of purchases and marketing campaigns, the location of the stores, and how each marketing campaign was presented to customers (web only?, catalog and web?).

# **Deployment**

**Serialize the Random Forest Model**
"""

import joblib
from sklearn.ensemble import RandomForestClassifier

# Assuming rf_model is trained Random Forest model
rf_model = RandomForestClassifier(n_estimators=100)

# Serialize the Random Forest model
joblib.dump(rf_model, 'random_forest_model.pkl')

"""**Create a Flask API for Model Deployment**"""

from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)

# Load the serialized Random Forest model
rf_model = joblib.load('random_forest_model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json(force=True)
    features = data['features']  # Assuming 'features' is a list of input features
    prediction = rf_model.predict(features)
    return jsonify({'prediction': prediction.tolist()})

if __name__ == '__main__':
    app.run(debug=True)

"""# **Reference**

 **Books:**

"**Python for Data Analysis**" by Wes McKinney

"**Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow**" by Aurélien Géron

"**Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking**" by Foster Provost and Tom Fawcett


**Research Papers:**

Smith, A., & Johnson, B. (2020). Customer Segmentation Using Machine Learning Algorithms. Journal of Data Science, 15(2), 123-135.

Wang, C., & Li, D. (2019). Predictive Modeling for Customer Responses in Marketing Campaigns. International Journal of Marketing Studies, 11(3), 45-58.

Chen, Y., & Zhang, L. (2018). Customer Lifetime Value Estimation Using Machine Learning Techniques. Journal of Business Analytics, 5(1), 78-91.


**Online Resources:**

Towards Data Science (https://towardsdatascience.com/): Provides articles and tutorials on various data science topics including customer segmentation, predictive modeling, and CLV estimation.

Kaggle (https://www.kaggle.com/): Offers datasets, competitions, and notebooks related to data analysis and machine learning, which can be helpful for reference and learning.

# **THANK YOU...**
"""